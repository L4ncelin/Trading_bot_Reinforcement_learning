{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour télécharger les données via yfinance\n",
    "def get_commodities_data(tickers, start=\"2010-01-01\", end=\"2023-01-01\", interval=\"1d\"):\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        df = yf.download(ticker, start=start, end=end, interval=interval)\n",
    "        data[ticker] = df\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes pour GC=F (Training, Validation, Test) : (2011, 5) (502, 5) (756, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "commodities = ['GC=F', 'SI=F', 'CL=F']\n",
    "\n",
    "commodities_data = get_commodities_data(commodities)\n",
    "\n",
    "# Définition des périodes\n",
    "training_data_time_range = ('2010-01-01', '2017-12-31')\n",
    "validation_data_time_range = ('2018-01-01', '2019-12-31')\n",
    "test_data_time_range = ('2020-01-01', '2023-01-01')\n",
    "\n",
    "# Création des jeux de données\n",
    "training_data = {}\n",
    "validation_data = {}\n",
    "test_data = {}\n",
    "for ticker, df in commodities_data.items():\n",
    "    training_data[ticker] = df.loc[training_data_time_range[0]:training_data_time_range[1]].copy()\n",
    "    validation_data[ticker] = df.loc[validation_data_time_range[0]:validation_data_time_range[1]].copy()\n",
    "    test_data[ticker] = df.loc[test_data_time_range[0]:test_data_time_range[1]].copy()\n",
    "\n",
    "print(\"Shapes pour GC=F (Training, Validation, Test) :\",\n",
    "training_data['GC=F'].shape, validation_data['GC=F'].shape, test_data['GC=F'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'ajout des indicateurs techniques\n",
    "def add_technical_indicators(df):\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "    df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    sma_tp = tp.rolling(window=20).mean()\n",
    "    mean_dev = tp.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
    "    df['CCI'] = (tp - sma_tp) / (0.015 * mean_dev)\n",
    "\n",
    "    high_diff = df['High'].diff()\n",
    "    low_diff = df['Low'].diff()\n",
    "    df['+DM'] = np.where((high_diff > low_diff) & (high_diff > 0), high_diff, 0)\n",
    "    df['-DM'] = np.where((low_diff > high_diff) & (low_diff > 0), low_diff, 0)\n",
    "    tr = pd.concat([df['High'] - df['Low'],\n",
    "    np.abs(df['High'] - df['Close'].shift(1)),\n",
    "    np.abs(df['Low'] - df['Close'].shift(1))], axis=1).max(axis=1)\n",
    "    atr = tr.ewm(span=14, adjust=False).mean()\n",
    "    df['+DI'] = 100 * (df['+DM'].ewm(span=14, adjust=False).mean() / atr)\n",
    "    df['-DI'] = 100 * (df['-DM'].ewm(span=14, adjust=False).mean() / atr)\n",
    "    dx = 100 * np.abs(df['+DI'] - df['-DI']) / (df['+DI'] + df['-DI'])\n",
    "    df['ADX'] = dx.ewm(span=14, adjust=False).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[['Open', 'High', 'Low', 'Close', 'Volume', 'MACD', 'Signal', 'RSI', 'CCI', 'ADX']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price              Open         High          Low        Close Volume  \\\n",
      "Ticker             GC=F         GC=F         GC=F         GC=F   GC=F   \n",
      "Date                                                                    \n",
      "2010-02-01  1081.000000  1107.300049  1076.800049  1104.300049   2452   \n",
      "2010-02-02  1117.400024  1118.500000  1100.199951  1117.400024   3326   \n",
      "2010-02-03  1115.800049  1124.900024  1108.599976  1111.400024    853   \n",
      "2010-02-04  1110.000000  1110.699951  1059.000000  1062.400024   1426   \n",
      "2010-02-05  1052.199951  1068.500000  1045.199951  1052.199951   1956   \n",
      "\n",
      "Price            MACD    Signal        RSI         CCI        ADX  \n",
      "Ticker                                                             \n",
      "Date                                                               \n",
      "2010-02-01  -8.662289 -5.106665  34.836633  -65.366222  28.495614  \n",
      "2010-02-02  -6.795084 -5.444349  46.015247  -10.964782  26.065814  \n",
      "2010-02-03  -5.733371 -5.502153  41.246502   -0.115025  25.602020  \n",
      "2010-02-04  -8.745038 -6.150730  28.394421 -114.395765  25.200066  \n",
      "2010-02-05 -11.818629 -7.284310  28.750695 -159.671611  24.851705  \n"
     ]
    }
   ],
   "source": [
    "# Application des indicateurs aux 3 jeux de données\n",
    "for ticker in training_data:\n",
    "    training_data[ticker] = add_technical_indicators(training_data[ticker])\n",
    "for ticker in validation_data:\n",
    "    validation_data[ticker] = add_technical_indicators(validation_data[ticker])\n",
    "for ticker in test_data:\n",
    "    test_data[ticker] = add_technical_indicators(test_data[ticker])\n",
    "\n",
    "print(training_data['GC=F'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'environnement de trading avec espace d'action discret\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class CommodityTradingEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    def __init__(self, commodity_data, transaction_cost_percent=0.005, initial_balance=10000):\n",
    "        super(CommodityTradingEnv, self).__init__()\n",
    "        # On garde uniquement les DataFrames non vides\n",
    "        self.commodity_data = {ticker: df for ticker, df in commodity_data.items() if not df.empty}\n",
    "        self.tickers = list(self.commodity_data.keys())\n",
    "        if not self.tickers:\n",
    "            raise ValueError(\"Aucune donnée disponible pour les commodities.\")\n",
    "        sample_df = next(iter(self.commodity_data.values()))\n",
    "        self.n_features = len(sample_df.columns)\n",
    "        # Définition d'un espace d'actions discret :\n",
    "        # Pour chaque commodity, l'action possible est dans la liste suivante :\n",
    "        self.action_list = [-1, -0.75, -0.50, -0.25, 0, 0.25, 0.50, 0.75, 1]\n",
    "        self.num_actions_per_commodity = len(self.action_list)\n",
    "        # L'action globale est une combinaison (codée en base-9)\n",
    "        self.action_space = spaces.Discrete(self.num_actions_per_commodity ** len(self.tickers))\n",
    "        # L'espace d'observation reste inchangé\n",
    "        self.obs_shape = self.n_features * len(self.tickers) + 2 + len(self.tickers) + 2\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_shape,), dtype=np.float32)\n",
    "        # Initialisation du portefeuille\n",
    "        self.initial_balance = initial_balance\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.max_net_worth = self.initial_balance\n",
    "        self.shares_held = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_shares_sold = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_sales_value = {ticker: 0 for ticker in self.tickers}\n",
    "        self.current_step = 0\n",
    "        self.max_steps = max(0, min(len(df) for df in self.commodity_data.values()) - 1)\n",
    "        self.transaction_cost_percent = transaction_cost_percent\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.max_net_worth = self.initial_balance\n",
    "        self.shares_held = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_shares_sold = {ticker: 0 for ticker in self.tickers}\n",
    "        self.total_sales_value = {ticker: 0 for ticker in self.tickers}\n",
    "        self.current_step = 0\n",
    "        return self._next_observation(), {}\n",
    "\n",
    "    def _next_observation(self):\n",
    "        frame = np.zeros(self.obs_shape)\n",
    "        idx = 0\n",
    "        for ticker in self.tickers:\n",
    "            df = self.commodity_data[ticker]\n",
    "            if self.current_step < len(df):\n",
    "                frame[idx:idx+self.n_features] = df.iloc[self.current_step].values\n",
    "            else:\n",
    "                frame[idx:idx+self.n_features] = df.iloc[-1].values\n",
    "        idx += self.n_features\n",
    "        # Ajout des informations supplémentaires\n",
    "        frame[-4-len(self.tickers)] = self.balance\n",
    "        frame[-3-len(self.tickers):-3] = [self.shares_held[ticker] for ticker in self.tickers]\n",
    "        frame[-3] = self.net_worth\n",
    "        frame[-2] = self.max_net_worth\n",
    "        frame[-1] = self.current_step\n",
    "        return frame\n",
    "\n",
    "    def decode_action(self, action):\n",
    "        \"\"\"Décode l'action discrète en un vecteur d'actions pour chaque commodity.\"\"\"\n",
    "        decoded_indices = []\n",
    "        temp = action\n",
    "        n = len(self.tickers)\n",
    "        for _ in range(n):\n",
    "            decoded_indices.append(temp % self.num_actions_per_commodity)\n",
    "            temp //= self.num_actions_per_commodity\n",
    "        decoded_indices.reverse()\n",
    "        decoded_actions = [self.action_list[idx] for idx in decoded_indices]\n",
    "        return decoded_actions\n",
    "\n",
    "    def step(self, action):\n",
    "        # Décodage de l'action discrète\n",
    "        actions = self.decode_action(action)\n",
    "        self.current_step += 1\n",
    "        if self.current_step > self.max_steps:\n",
    "            return self._next_observation(), 0, True, False, {}\n",
    "\n",
    "        current_prices = {}\n",
    "        for i, ticker in enumerate(self.tickers):\n",
    "            # Correction FutureWarning : accès direct à la colonne 'Close' pour obtenir une valeur scalaire\n",
    "            current_prices[ticker] = float(self.commodity_data[ticker]['Close'].iloc[self.current_step])\n",
    "            act = actions[i]\n",
    "\n",
    "        if act > 0: # Achat\n",
    "            shares_to_buy = int(self.balance * act / current_prices[ticker])\n",
    "            cost = shares_to_buy * current_prices[ticker]\n",
    "            transaction_cost = cost * self.transaction_cost_percent\n",
    "            self.balance -= (cost + transaction_cost)\n",
    "            self.shares_held[ticker] += shares_to_buy\n",
    "        elif act < 0: # Vente\n",
    "            shares_to_sell = int(self.shares_held[ticker] * abs(act))\n",
    "            sale = shares_to_sell * current_prices[ticker]\n",
    "            transaction_cost = sale * self.transaction_cost_percent\n",
    "            self.balance += (sale - transaction_cost)\n",
    "            self.shares_held[ticker] -= shares_to_sell\n",
    "            self.total_shares_sold[ticker] += shares_to_sell\n",
    "            self.total_sales_value[ticker] += sale\n",
    "\n",
    "        self.net_worth = self.balance + sum(self.shares_held[ticker] * current_prices[ticker] for ticker in self.tickers)\n",
    "        self.max_net_worth = max(self.net_worth, self.max_net_worth)\n",
    "        reward = self.net_worth - self.initial_balance\n",
    "        done = self.net_worth <= 0 or self.current_step >= self.max_steps\n",
    "\n",
    "        return self._next_observation(), reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        profit = self.net_worth - self.initial_balance\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Balance: {self.balance:.2f}\")\n",
    "        for ticker in self.tickers:\n",
    "            print(f\"{ticker} Shares held: {self.shares_held[ticker]}\")\n",
    "            print(f\"Net worth: {self.net_worth:.2f} | Profit: {profit:.2f}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "\n",
      "=== Epoch 1/200 ===\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimesteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Évaluation sur l'environnement de validation (un épisode)\u001b[39;00m\n\u001b[32m     23\u001b[39m     val_obs = env_val.reset() \u001b[38;5;66;03m# Modification : on récupère directement l'observation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Developpement\\Miniconda\\envs\\env_rl_project\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[39m, in \u001b[36mDQN.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    259\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[32m    260\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    265\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    266\u001b[39m ) -> SelfDQN:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Developpement\\Miniconda\\envs\\env_rl_project\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:314\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[32m    307\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    313\u001b[39m ) -> SelfOffPolicyAlgorithm:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     total_timesteps, callback = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     callback.on_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mYou must set the environment before calling learn()\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Developpement\\Miniconda\\envs\\env_rl_project\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:297\u001b[39m, in \u001b[36mOffPolicyAlgorithm._setup_learn\u001b[39m\u001b[34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.num_envs > \u001b[32m1\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_noise, VectorizedActionNoise)\n\u001b[32m    294\u001b[39m ):\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_noise = VectorizedActionNoise(\u001b[38;5;28mself\u001b[39m.action_noise, \u001b[38;5;28mself\u001b[39m.env.num_envs)\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Developpement\\Miniconda\\envs\\env_rl_project\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:432\u001b[39m, in \u001b[36mBaseAlgorithm._setup_learn\u001b[39m\u001b[34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._custom_logger:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[32m    435\u001b[39m callback = \u001b[38;5;28mself\u001b[39m._init_callback(callback, progress_bar)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Developpement\\Miniconda\\envs\\env_rl_project\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:202\u001b[39m, in \u001b[36mconfigure_logger\u001b[39m\u001b[34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[39m\n\u001b[32m    199\u001b[39m save_path, format_strings = \u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[33m\"\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTrying to log data to tensorboard but tensorboard is not installed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    205\u001b[39m     latest_run_id = get_latest_run_id(tensorboard_log, tb_log_name)\n",
      "\u001b[31mImportError\u001b[39m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "source": [
    "# Entraînement avec Double DQN (via DQN de Stable Baselines3) en utilisant cuda\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Création des environnements d'entraînement, de validation et de test\n",
    "env_train = DummyVecEnv([lambda: CommodityTradingEnv(training_data, transaction_cost_percent=0.005)])\n",
    "env_val = DummyVecEnv([lambda: CommodityTradingEnv(validation_data, transaction_cost_percent=0.005)])\n",
    "env_test = DummyVecEnv([lambda: CommodityTradingEnv(test_data, transaction_cost_percent=0.005)])\n",
    "\n",
    "# Création du modèle DQN avec GPU (cuda)\n",
    "model = DQN(\"MlpPolicy\", env_train, verbose=1, tensorboard_log=\"./dqn_tensorboard/\", device=\"cuda\")\n",
    "\n",
    "# Boucle d'entraînement avec sauvegarde du meilleur modèle (selon la récompense cumulée sur validation)\n",
    "best_val_reward = -np.inf\n",
    "best_model_path = \"best_dqn_model.zip\"\n",
    "num_epochs = 200\n",
    "timesteps_per_epoch = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n=== Epoch {epoch+1}/{num_epochs} ===\")\n",
    "    model.learn(total_timesteps=timesteps_per_epoch, reset_num_timesteps=False)\n",
    "    # Évaluation sur l'environnement de validation (un épisode)\n",
    "    val_obs = env_val.reset() # Modification : on récupère directement l'observation\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(val_obs)\n",
    "        val_obs, reward, done, info = env_val.step(action)\n",
    "        cumulative_reward += reward[0]\n",
    "        print(f\"Récompense cumulée sur validation : {cumulative_reward}\")\n",
    "        if cumulative_reward > best_val_reward:\n",
    "            best_val_reward = cumulative_reward\n",
    "            model.save(best_model_path)\n",
    "            print(f\"Nouveau meilleur modèle sauvegardé (reward = {best_val_reward})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du meilleur modèle sur l'environnement de test\n",
    "best_model = DQN.load(best_model_path, env=env_test)\n",
    "obs = env_test.reset() # Modification : récupération directe de l'observation\n",
    "done = False\n",
    "test_cumulative_reward = 0\n",
    "while not done:\n",
    "    action, _ = best_model.predict(obs)\n",
    "    obs, reward, done, info = env_test.step(action)\n",
    "    test_cumulative_reward += reward[0]\n",
    "    print(\"\\nRécompense cumulée sur test :\", test_cumulative_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionnel) Visualisation de l'évolution du Net Worth sur le set de test\n",
    "def test_agent(env, model, n_tests=1000, visualize=False):\n",
    "    metrics = {\n",
    "    'steps': [],\n",
    "    'balances': [],\n",
    "    'net_worths': [],\n",
    "    'shares_held': {ticker: [] for ticker in env.envs[0].commodity_data.keys()}\n",
    "    }\n",
    "    obs = env.reset() # Modification ici également\n",
    "    for i in range(n_tests):\n",
    "        metrics['steps'].append(i)\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if visualize:\n",
    "            env.envs[0].render()\n",
    "            metrics['balances'].append(env.envs[0].balance)\n",
    "            metrics['net_worths'].append(env.envs[0].net_worth)\n",
    "        for ticker in env.envs[0].commodity_data.keys():\n",
    "            metrics['shares_held'][ticker].append(env.envs[0].shares_held[ticker])\n",
    "        if done:\n",
    "            obs = env.reset() # Modification ici\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = test_agent(env_test, best_model, n_tests=1000, visualize=False)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_metrics['steps'], test_metrics['net_worths'], label=\"Net Worth\")\n",
    "plt.title(\"Évolution du Net Worth sur le set de Test\")\n",
    "plt.xlabel(\"Étapes\")\n",
    "plt.ylabel(\"Net Worth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
